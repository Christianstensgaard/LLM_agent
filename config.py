LLM_CONFIG = {
    "config_list": [
        {
            "model": "llama3.2",
            "client_host": "http://127.0.0.1:11434",
            "api_type": "ollama",
            "repeat_penalty": 1.1,
            "stream": False,
            "seed": 42
        }
    ]
}
